{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNqeb0z7bE0yFuziCnP5KMJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mayur619/avc-detection/blob/notebooks/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKCMSj8aSbNc"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical,plot_model\n",
        "from tensorflow.keras.layers import Conv2D,Activation,Input,Concatenate,Flatten,Dense,GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy, MeanSquaredLogarithmicError\n",
        "from tensorflow.keras.applications import MobileNetV2,EfficientNetB0\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E5ancKiUDs0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50f90f40-f797-4d33-972f-178a51fee149"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGE9IB0aUFk6"
      },
      "source": [
        "DATA_PATH = \"/content/drive/My Drive/AnimalVehicleCollision/data\"\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 8\n",
        "ANIMAL_LABELS_DICT = {0:\"cow\",1:\"dog\",2:\"bear\"}\n",
        "WIDTH=224\n",
        "HEIGHT=224"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nNJ5-PrUwYB"
      },
      "source": [
        "class AnimalDataGenerator:\n",
        "  def __init__(self,base_path,csv_path,batch_size):\n",
        "    self.index=0\n",
        "    self.base_path=base_path\n",
        "    self.batch_size=batch_size\n",
        "    self.df=pd.read_csv(csv_path)\n",
        "  def __load_images(self,image_name_list,label_list):\n",
        "    images=[]\n",
        "    for i,image_name in enumerate(image_name_list):\n",
        "      image=cv2.imread(os.path.join(self.base_path,ANIMAL_LABELS_DICT[label_list[i]],image_name))\n",
        "      image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "      image=cv2.resize(image,(HEIGHT,WIDTH))\n",
        "      images.append(image)\n",
        "    return np.array(images)\n",
        "  def get_steps_per_epoch(self):\n",
        "    return self.df.shape[0]//self.batch_size\n",
        "  def generate_image_batches(self,is_training):\n",
        "    while True:\n",
        "      for i in range(self.df.shape[0]//self.batch_size):\n",
        "        bounding_boxes=self.df.iloc[self.index:self.index+self.batch_size,2:6].values\n",
        "        animal_labels=self.df.iloc[self.index:self.index+self.batch_size,6].values\n",
        "        images=self.__load_images(self.df.iloc[self.index:self.index+self.batch_size,1].values,animal_labels)\n",
        "        animal_labels=to_categorical(animal_labels,num_classes=len(ANIMAL_LABELS_DICT.keys()))\n",
        "        orientation=to_categorical(self.df.iloc[self.index:self.index+self.batch_size,-1].values,num_classes=4)\n",
        "        path=self.df.iloc[self.index:self.index+self.batch_size,7:9].values\n",
        "        yield [images,bounding_boxes,animal_labels],[orientation,path]\n",
        "        self.index+=self.batch_size\n",
        "        del animal_labels,bounding_boxes,images,orientation,path\n",
        "      self.index=0"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUXMRLW3fbAb"
      },
      "source": [
        "def cnn_model():\n",
        "  input=Input(shape=(WIDTH,HEIGHT,3,),name=\"image_input\")\n",
        "  model=Conv2D(16,(3,3),padding='same')(input)\n",
        "  model=Activation('relu')(model)\n",
        "  model=Flatten()(model)\n",
        "  model=Dense(16)(model)\n",
        "  return input,model"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGYcVMmKnYDg"
      },
      "source": [
        "def cnn_mobilenetv2():\n",
        "  base_model=MobileNetV2(input_shape=(WIDTH,HEIGHT,3),weights='imagenet',include_top=False)\n",
        "  for layer in base_model.layers:\n",
        "    layer.trainable=False\n",
        "  model=GlobalAveragePooling2D()(base_model.output)\n",
        "  model=Activation('relu')(model)\n",
        "  model=Flatten()(model)\n",
        "  model=Dense(16)(model)\n",
        "  return base_model.input, model"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJUFc3CF2gGr"
      },
      "source": [
        "def cnn_efficientnetb0():\n",
        "  base_model=EfficientNetB0(include_top=False,input_shape=(WIDTH,HEIGHT,3))\n",
        "  for layer in base_model.layers:\n",
        "    layer.trainable=False\n",
        "  model=GlobalAveragePooling2D()(base_model.output)\n",
        "  model=Activation('relu')(model)\n",
        "  model=Flatten()(model)\n",
        "  model=Dense(16)(model)\n",
        "  return base_model.input, model"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMX3PCYugIVr"
      },
      "source": [
        "def bounding_box_model():\n",
        "  input=Input(shape=(4,),name=\"bounding_box_input\")\n",
        "  model=Dense(8)(input)\n",
        "  model=Activation('relu')(model)\n",
        "  model=Flatten()(model)\n",
        "  return input,model"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8J7m65RgbXm"
      },
      "source": [
        "def animal_label_model():\n",
        "  input=Input(shape=(len(ANIMAL_LABELS_DICT.keys()),),name=\"animal_label_input\")\n",
        "  model=Dense(8)(input)\n",
        "  model=Activation('relu')(model)\n",
        "  model=Flatten()(model)\n",
        "  return input,model"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn2dbH8WqEcG"
      },
      "source": [
        "#cnn_input,cnn_output=cnn_model()\n",
        "#cnn_input,cnn_output=cnn_mobilenetv2()\n",
        "cnn_input,cnn_output=cnn_efficientnetb0()\n",
        "bounding_box_input,bounding_box_output=bounding_box_model()\n",
        "animal_label_input,animal_label_output=animal_label_model()\n",
        "core_model=Concatenate(axis=1)([cnn_output,bounding_box_output,animal_label_output])\n",
        "core_model=Dense(64)(core_model)\n",
        "orientation=Dense(4,activation='softmax',name='orientation_output')(core_model)\n",
        "path=Dense(2,name='path_output')(core_model)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5qeUiaXqwIA"
      },
      "source": [
        "model=Model(inputs=[cnn_input,bounding_box_input,animal_label_input],outputs=[orientation,path])"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-Tvep8ArPnD"
      },
      "source": [
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss={\n",
        "                  'orientation_output':CategoricalCrossentropy(from_logits=True),\n",
        "                  'path_output':MeanSquaredLogarithmicError()\n",
        "              })"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6AqS-GDyIi6"
      },
      "source": [
        "train_generator=AnimalDataGenerator(os.path.join(DATA_PATH,\"images\"),os.path.join(DATA_PATH,\"train.csv\"),BATCH_SIZE)\n",
        "test_generator=AnimalDataGenerator(os.path.join(DATA_PATH,\"images\"),os.path.join(DATA_PATH,\"test.csv\"),BATCH_SIZE)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDxHONbXyU0E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0f30431-4c00-4f3c-8ca5-4b558825ebe2"
      },
      "source": [
        "model.fit(train_generator.generate_image_batches(True),\n",
        "          epochs=3,\n",
        "          steps_per_epoch=train_generator.get_steps_per_epoch(),\n",
        "          validation_data=test_generator.generate_image_batches(True),\n",
        "          validation_steps=test_generator.get_steps_per_epoch())"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "421/421 [==============================] - 265s 628ms/step - loss: 1.1023 - orientation_output_loss: 1.0813 - path_output_loss: 0.0211 - val_loss: 1.1272 - val_orientation_output_loss: 1.1044 - val_path_output_loss: 0.0228\n",
            "Epoch 2/3\n",
            "421/421 [==============================] - 246s 584ms/step - loss: 1.0939 - orientation_output_loss: 1.0731 - path_output_loss: 0.0208 - val_loss: 1.1252 - val_orientation_output_loss: 1.1023 - val_path_output_loss: 0.0229\n",
            "Epoch 3/3\n",
            "421/421 [==============================] - 249s 593ms/step - loss: 1.0908 - orientation_output_loss: 1.0701 - path_output_loss: 0.0207 - val_loss: 1.1258 - val_orientation_output_loss: 1.1027 - val_path_output_loss: 0.0231\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb5737f6b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhJ0m0e3thvj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}